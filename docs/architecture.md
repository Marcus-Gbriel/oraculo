# Arquitetura do Sistema Or√°culo

## üèõÔ∏è Vis√£o Geral

O Sistema Or√°culo utiliza uma arquitetura moderna de RAG (Retrieval Augmented Generation) implementada de forma totalmente local, seguindo princ√≠pios de Clean Architecture e SOLID.

## üìê Diagrama de Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        index.py                             ‚îÇ
‚îÇ                   (Entry Point / CLI)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  modules/oracle_system.py                   ‚îÇ
‚îÇ              (Orchestrator / RAG Pipeline)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ         ‚îÇ
       ‚ñº          ‚ñº          ‚ñº          ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Document ‚îÇ ‚îÇ  Text  ‚îÇ ‚îÇEmbedding‚îÇ ‚îÇ Vector ‚îÇ ‚îÇLLM ‚îÇ
‚îÇ  Loader  ‚îÇ ‚îÇProcessor‚îÇ ‚îÇGenerator‚îÇ ‚îÇ Store  ‚îÇ ‚îÇ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ          ‚îÇ           ‚îÇ         ‚îÇ
     ‚ñº            ‚ñº          ‚ñº           ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PyPDF2  ‚îÇ ‚îÇ  Regex ‚îÇ ‚îÇsentence-‚îÇ ‚îÇChroma  ‚îÇ ‚îÇllama‚îÇ
‚îÇ   docx   ‚îÇ ‚îÇ String ‚îÇ ‚îÇtransfor-‚îÇ ‚îÇ  DB    ‚îÇ ‚îÇ.cpp‚îÇ
‚îÇ openpyxl ‚îÇ ‚îÇ  Ops   ‚îÇ ‚îÇ  mers   ‚îÇ ‚îÇ        ‚îÇ ‚îÇ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîÑ Fluxo de Dados

### 1. Fase de Indexa√ß√£o

```
Documentos (PDF/DOCX/Excel)
         ‚Üì
   DocumentLoader
         ‚Üì
   Texto Extra√≠do
         ‚Üì
   TextProcessor (Chunking)
         ‚Üì
   Chunks de Texto
         ‚Üì
   EmbeddingGenerator
         ‚Üì
   Vetores (Embeddings)
         ‚Üì
   VectorStore (ChromaDB)
         ‚Üì
   Base de Conhecimento Indexada
```

### 2. Fase de Consulta (Query)

```
Pergunta do Usu√°rio
         ‚Üì
   EmbeddingGenerator
         ‚Üì
   Vetor da Pergunta
         ‚Üì
   VectorStore.search()
         ‚Üì
   Top-K Documentos Relevantes
         ‚Üì
   LocalLLM.create_prompt_with_context()
         ‚Üì
   Prompt Contextualizado
         ‚Üì
   LocalLLM.generate()
         ‚Üì
   Resposta Final
```

## üß© Componentes Principais

### 1. index.py (Entry Point)

**Responsabilidades:**
- Interface CLI do usu√°rio
- Verifica√ß√£o de depend√™ncias
- Inicializa√ß√£o do sistema
- Menu interativo
- Tratamento de erros globais

**Padr√µes:**
- Single Responsibility Principle
- Command Pattern (menu)

### 2. oracle_system.py (Orchestrator)

**Responsabilidades:**
- Coordena√ß√£o de todos os m√≥dulos
- Pipeline RAG completo
- Gerenciamento de estado
- API p√∫blica do sistema

**Padr√µes:**
- Facade Pattern
- Pipeline Pattern
- Dependency Injection

**M√©todos P√∫blicos:**
```python
__init__(documents_path, vectorstore_path, model_path, use_simple_llm)
index_documents(force_reindex)
query(question, n_results, show_sources)
interactive_mode()
get_stats()
```

### 3. document_loader.py (Data Ingestion)

**Responsabilidades:**
- Leitura de m√∫ltiplos formatos
- Extra√ß√£o de texto
- Tratamento de erros por arquivo
- Logging de progresso

**Formatos Suportados:**
- PDF (PyPDF2)
- DOCX (python-docx)
- Excel XLSX/XLS (openpyxl)

**Interface:**
```python
DocumentLoader(documents_path)
load_pdf(file_path) -> str
load_docx(file_path) -> str
load_excel(file_path) -> str
load_all_documents() -> List[Dict]
```

### 4. text_processor.py (Text Processing)

**Responsabilidades:**
- Limpeza de texto
- Chunking com overlap
- Manuten√ß√£o de contexto
- Metadados por chunk

**Estrat√©gia de Chunking:**
- Tamanho configur√°vel (default: 500 chars)
- Overlap configur√°vel (default: 50 chars)
- Quebra em espa√ßos (n√£o corta palavras)
- Preserva metadados de origem

**Interface:**
```python
TextProcessor(chunk_size, chunk_overlap)
clean_text(text) -> str
split_into_sentences(text) -> List[str]
create_chunks(text, metadata) -> List[Dict]
process_documents(documents) -> List[Dict]
```

### 5. embedding_generator.py (Embeddings)

**Responsabilidades:**
- Gera√ß√£o de embeddings sem√¢nticos
- Cache de modelo
- Processamento em batch
- Suporte multil√≠ngue

**Modelo Padr√£o:**
- `paraphrase-multilingual-MiniLM-L12-v2`
- 384 dimens√µes
- Suporta 50+ idiomas incluindo portugu√™s

**Interface:**
```python
EmbeddingGenerator(model_name)
generate_embedding(text) -> np.ndarray
generate_embeddings_batch(texts, batch_size) -> List[np.ndarray]
```

### 6. vector_store.py (Vector Database)

**Responsabilidades:**
- Armazenamento persistente de vetores
- Busca por similaridade (cosine)
- Gerenciamento de cole√ß√µes
- Batch operations

**Tecnologia:**
- ChromaDB (vector database local)
- Dist√¢ncia: Cosine Similarity
- Persist√™ncia em disco

**Interface:**
```python
VectorStore(persist_directory, collection_name)
add_documents(chunks, embeddings)
search(query_embedding, n_results) -> List[Dict]
clear_collection()
get_collection_stats() -> Dict
```

### 7. llm.py (Language Models)

**Responsabilidades:**
- Gerenciamento de m√∫ltiplos tipos de LLM
- Gera√ß√£o de respostas
- Cria√ß√£o de prompts contextualizados
- Detec√ß√£o e uso de GPU
- Fallback autom√°tico entre modelos

**Modelos Suportados:**
- **GPT4AllLLM** (Padr√£o): 7 modelos pr√©-configurados
- **LocalLLM**: llama-cpp-python (legado)
- **SimpleLLM**: Modo teste

**Otimiza√ß√µes:**
- Temperature: 0.2 (precis√£o)
- GPU auto-detection (CUDA)
- Multi-threading CPU
- Batch processing (n_batch=256)

**Interface:**
```python
GPT4AllLLM(model_name, temperature, max_tokens, use_gpu)
generate(prompt, max_tokens, temperature) -> str
create_prompt_with_context(question, context_documents) -> str
detect_gpu() -> bool
```

### 8. model_manager.py (Model Management)

**Responsabilidades:**
- Gerenciamento de modelos LLM dispon√≠veis
- Persist√™ncia de configura√ß√µes (config.json)
- Listagem e sele√ß√£o de modelos
- Valida√ß√£o de modelos instalados

**Modelos Dispon√≠veis:**
1. Mistral 7B OpenOrca (3.8 GB)
2. Mistral 7B Instruct (3.8 GB)
3. Orca 2 7B (3.8 GB)
4. Nous Hermes LLaMA2 13B (7.3 GB)
5. GPT4All Falcon (3.9 GB)
6. WizardLM 13B (7.3 GB)
7. Orca Mini 3B (1.8 GB)

**Configura√ß√£o (config.json):**
```json
{
  "selected_model": "mistral-7b-openorca.Q4_0.gguf",
  "temperature": 0.2,
  "max_tokens": 512
}
```

**Interface:**
```python
ModelManager(models_dir, config_file)
get_installed_models() -> List[str]
get_selected_model() -> str
set_selected_model(model_name)
list_available_models() -> Dict
print_all_models()
print_model_info(model_file)
```

## üéØ Padr√µes de Design Utilizados

### 1. Facade Pattern
- `OracleSystem` como facade para todo o sistema
- Simplifica interface complexa

### 2. Strategy Pattern
- `LocalLLM` vs `SimpleLLM`
- Diferentes estrat√©gias de gera√ß√£o

### 3. Template Method
- `DocumentLoader` com m√©todos espec√≠ficos por formato
- Estrutura comum com varia√ß√µes

### 4. Dependency Injection
- Componentes injetados no `OracleSystem`
- Facilita testes e manuten√ß√£o

### 5. Single Responsibility
- Cada m√≥dulo tem uma responsabilidade clara
- Alto coes√£o, baixo acoplamento

## üìä Fluxo de Dados Detalhado

### Indexa√ß√£o (Primeira Execu√ß√£o)

```python
# 1. Carregar documentos
docs = DocumentLoader("training").load_all_documents()
# Output: [{'filename': 'doc.pdf', 'content': '...', 'path': '...'}]

# 2. Processar em chunks
processor = TextProcessor(chunk_size=500, chunk_overlap=50)
chunks = processor.process_documents(docs)
# Output: [{'text': '...', 'metadata': {...}, 'chunk_index': 0}]

# 3. Gerar embeddings
embedder = EmbeddingGenerator()
embeddings = embedder.generate_embeddings_batch([c['text'] for c in chunks])
# Output: [array([0.1, 0.2, ...]), ...]  # 384 dimens√µes

# 4. Armazenar
vector_store = VectorStore("src/vectorstore")
vector_store.add_documents(chunks, embeddings)
# Persistido em disco automaticamente
```

### Consulta (Query)

```python
# 1. Receber pergunta
question = "O que √© desenvolvimento seguro?"

# 2. Gerar embedding da pergunta
q_embedding = embedder.generate_embedding(question)
# Output: array([0.15, 0.23, ...])  # 384 dimens√µes

# 3. Buscar documentos similares
relevant_docs = vector_store.search(q_embedding, n_results=3)
# Output: [
#   {'text': '...', 'metadata': {...}, 'distance': 0.23},
#   {'text': '...', 'metadata': {...}, 'distance': 0.31},
#   {'text': '...', 'metadata': {...}, 'distance': 0.35}
# ]

# 4. Criar prompt contextualizado
prompt = llm.create_prompt_with_context(question, relevant_docs)
# Output: "<s>[INST] ... Contexto: ... Pergunta: ... [/INST]"

# 5. Gerar resposta
response = llm.generate(prompt)
# Output: "Desenvolvimento seguro √©..."
```

## üíæ Persist√™ncia

### Vector Store
- **Local**: `src/vectorstore/`
- **Formato**: SQLite + arquivos bin√°rios (ChromaDB)
- **Tamanho**: ~1-5KB por chunk

### Logs
- **Local**: `oraculo.log`
- **Rota√ß√£o**: Manual
- **Formato**: Timestamp + Level + Message

### Modelos
- **Embeddings**: Cache autom√°tico em `~/.cache/torch/`
- **LLM**: `src/models/*.gguf` (usu√°rio fornece)

## üîí Seguran√ßa e Privacidade

### Princ√≠pios
1. **Dados Locais**: Nunca saem da m√°quina
2. **Sem Telemetria**: ChromaDB configurado sem telemetria
3. **Sem Rede**: Funciona 100% offline ap√≥s instala√ß√£o
4. **Open Source**: C√≥digo audit√°vel

### Considera√ß√µes
- Documentos ficam em `training/` (n√£o criptografados)
- Vector store n√£o √© criptografado
- Logs podem conter trechos de documentos
- Para ambiente de produ√ß√£o, considere criptografia de disco

## ‚ö° Performance

### Otimiza√ß√µes

1. **Batch Processing**
   - Embeddings gerados em batches de 32
   - Reduz overhead de chamadas ao modelo

2. **Vector Search**
   - HNSW (Hierarchical Navigable Small World)
   - Complexidade O(log n) para busca

3. **Chunking Inteligente**
   - Quebra em espa√ßos (mant√©m palavras completas)
   - Overlap preserva contexto

4. **Lazy Loading**
   - Modelos carregados apenas quando necess√°rio
   - Cache autom√°tico de embeddings

### Benchmarks T√≠picos

| Opera√ß√£o | Tempo | Hardware |
|----------|-------|----------|
| Indexar 10 PDFs (100 p√°ginas) | 2-5 min | CPU i5, 8GB RAM |
| Gerar embedding de query | 50-200ms | CPU i5 |
| Busca no vector store | 10-50ms | 1000 docs |
| Gerar resposta LLM | 5-30s | CPU i5 (Q4 model) |

## üß™ Testes

### Estrutura Sugerida

```
tests/
‚îú‚îÄ‚îÄ test_document_loader.py
‚îú‚îÄ‚îÄ test_text_processor.py
‚îú‚îÄ‚îÄ test_embedding_generator.py
‚îú‚îÄ‚îÄ test_vector_store.py
‚îú‚îÄ‚îÄ test_llm.py
‚îî‚îÄ‚îÄ test_oracle_system.py
```

### Testes Unit√°rios

```python
# Exemplo
def test_document_loader():
    loader = DocumentLoader("test_data")
    docs = loader.load_all_documents()
    assert len(docs) > 0
    assert 'content' in docs[0]
```

### Testes de Integra√ß√£o

```python
def test_full_pipeline():
    oracle = OracleSystem("test_data")
    oracle.index_documents()
    response = oracle.query("teste")
    assert response is not None
```

## üîÑ Extensibilidade

### Adicionar Novo Formato de Documento

```python
# Em document_loader.py
def load_txt(self, file_path: Path) -> str:
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

# Adicionar ao __init__
self.supported_formats = ['.pdf', '.docx', '.xlsx', '.xls', '.txt']

# Adicionar ao load_all_documents
elif file_path.suffix.lower() == '.txt':
    content = self.load_txt(file_path)
```

### Trocar Vector Database

```python
# Criar novo m√≥dulo: modules/faiss_store.py
class FAISSStore:
    def __init__(self, persist_directory):
        # Implementa√ß√£o com FAISS
        pass
    
    def add_documents(self, chunks, embeddings):
        pass
    
    def search(self, query_embedding, n_results):
        pass
```

### Adicionar Novo Modelo de Embeddings

```python
# Em embedding_generator.py
class HuggingFaceEmbedding(EmbeddingGenerator):
    def __init__(self):
        from transformers import AutoModel, AutoTokenizer
        self.model = AutoModel.from_pretrained("model-name")
        self.tokenizer = AutoTokenizer.from_pretrained("model-name")
```

## üìà Escalabilidade

### Limites Atuais
- **Documentos**: At√© ~10.000 documentos (depende do tamanho)
- **Chunks**: At√© ~100.000 chunks no vector store
- **Mem√≥ria LLM**: 4-16GB dependendo do modelo

### Para Escalar
1. Usar PostgreSQL + pgvector ao inv√©s de ChromaDB
2. Implementar sharding de collections
3. Usar GPU para embeddings e LLM
4. Adicionar cache de respostas
5. Implementar processamento ass√≠ncrono

## üéì Conceitos T√©cnicos

### RAG (Retrieval Augmented Generation)
T√©cnica que combina:
1. **Retrieval**: Busca de informa√ß√£o relevante
2. **Augmentation**: Enriquecimento do prompt
3. **Generation**: Gera√ß√£o de resposta contextualizada

### Embeddings
Representa√ß√£o vetorial de texto que captura significado sem√¢ntico:
- Textos similares = vetores pr√≥ximos
- Permite busca por similaridade
- Multil√≠ngue: mesmo espa√ßo vetorial para v√°rios idiomas

### Vector Database
Banco de dados otimizado para busca de similaridade:
- √çndices especializados (HNSW, IVF)
- M√©tricas de dist√¢ncia (cosine, euclidean)
- Busca aproximada (ANN - Approximate Nearest Neighbors)

### Quantiza√ß√£o (GGUF)
Redu√ß√£o de precis√£o de modelos LLM:
- FP16 ‚Üí INT8/INT4 (Q4, Q5, Q8)
- Reduz tamanho e requisitos de mem√≥ria
- Pequena perda de qualidade

## üìö Refer√™ncias

- **sentence-transformers**: https://www.sbert.net/
- **ChromaDB**: https://docs.trychroma.com/
- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **RAG**: https://arxiv.org/abs/2005.11401
- **HNSW**: https://arxiv.org/abs/1603.09320

---

**Sistema Or√°culo v1.0.0** - Arquitetura Profissional para RAG Local
